
:toc:
:toc-placement!:
:toclevels: 4

= Setting up a cluster manually

This is a guide that explains step by step how you can install your
own Kubernetes cluster on bare metal servers with somewhat limited
resources. Wherever possible I've linked to the origin of the information
so you can read things for yourself.

And the first link is to the guide that *I* used myself to help me set up
my Kubernetes cluster. After looking around and reading several guides I
finally decided on this one written by Patrick Stadler:

*Kubernetes clusters for the hobbyist*:
https://github.com/hobby-kube/guide[https://github.com/hobby-kube/guide]

It goes into a lot of detail and often explains the "why" behind the
decisions he made.

But it's obviously it was still going to be quite a complex undertaking.
So I decided to keep a "log" of all the steps I was taking along the
way. And bit by bit this turned somewhat into a guide itself. Still very
much based on Patrick's guide, but different enough to perhaps make it
still worthwhile. Just keep reading...

toc::[]

==== Server setup

First we need to do the very important job of ...

xref:prepare.adoc[Preparing the Servers]!

Don't continue here until that is done!

Okay.

Now that are servers are partitioned and rebooted we can log into them
to do the rest of the work. I'd suggest opening up 3 terminal windows,
one for each server, because many actions will have to be duplicated
exactly or in a very similart way at least on each server. It's easier
to just copy once and paste 3 times and let them work in parallel.

==== Host names

....
ssh root@<NODE_NAME>.k8s.example.com
....

But this time use the password sent to you by the hosting provider.

Now the first thing we'll do is set the hostname for each node. So on
each server (look at the names above the panels) execute the
corresponding command.

*Kube1*

....
echo "kube1" > /etc/hostname
....

*Kube2*

....
echo "kube2" > /etc/hostname
....

*Kube3*

....
echo "kube3" > /etc/hostname
....

Let’s also update `/etc/hosts` with our public IP and FQN host name.
Either add the following line or replace the existing line with it:

....
<NODE_IP>       <NODE_NAME>.k8s.example.com
....

And now we reboot again. I'm not sure if this is really necessary, but I
had a couple of times that certain parts of the system kept insisting
that the name of the server wasn't `kubeX` but some name generated by
the hosting provder. Rebooting seemed to help, so just to be sure, type:

....
reboot
....

==== Get rid of swap

The next thing we’ll do is turn off swap which is something Kubernetes
doesn’t like much. Run this command on each server:

....
swapoff -a
....

Now edit the `/etc/fstab` file and remove the line that mentions “swap”.

==== Some miscellaneous bits

Just making sure that some commands further on in this guide will work.
Run these on each node.

....
# Making sure apt-add-repository is installed
apt update
apt install software-properties-common
# IP forwarding needs to be enabled
echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf
sysctl -p
....

==== Installing Docker

To install and set up Docker we’ll have to run the following on each
node:

....
apt-get install docker.io
....

And set some options by running the following:

....
mkdir -p /etc/systemd/system/docker.service.d
cat > /etc/systemd/system/docker.service.d/10-docker-opts.conf <<EOF
Environment="DOCKER_OPTS=--iptables=false --ip-masq=false"
EOF
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
....

And finally:

....
systemctl daemon-reload
systemctl restart docker
systemctl enable docker.service
....

=== Network & Security

==== Firewall

We’ll be following the instruction from the guide to set up the firewall
on each of the servers using
https://wiki.ubuntu.com/UncomplicatedFirewall[UFW]:

....
ufw allow ssh
ufw allow 6443
ufw allow 80
ufw allow 443
ufw default deny incoming
ufw enable
....

==== Wireguard

Now let’s install https://www.wireguard.com/install/[Wireguard] on all
nodes. And because we’re using Ubuntu 18 we have to set up the
repository first.

....
add-apt-repository ppa:wireguard/wireguard
apt install wireguard
....

Now on the each server we’ll create a file with some configuration to
tell WireGuard how to connect to its peers. Run each of these scipts on
their corresponding nodes:

*Kube1*

....
cat > /etc/wireguard/wg0.conf <<EOF
[Interface]
Address = 10.0.1.1
PrivateKey = <PRIVATE_KEY_KUBE1>
ListenPort = 51820

[Peer]
PublicKey = <PUBLIC_KEY_KUBE2>
AllowedIps = 10.0.1.2/32
Endpoint = 22.22.22.222:51820

[Peer]
PublicKey = <PUBLIC_KEY_KUBE3>
AllowedIps = 10.0.1.3/32
Endpoint = 33.33.33.333:51820
EOF
....

*Kube2*

....
cat > /etc/wireguard/wg0.conf <<EOF
[Interface]
Address = 10.0.1.2
PrivateKey = <PRIVATE_KEY_KUBE2>
ListenPort = 51820

[Peer]
PublicKey = <PUBLIC_KEY_KUBE1>
AllowedIps = 10.0.1.1/32
Endpoint = 11.11.11.111:51820

[Peer]
PublicKey = <PUBLIC_KEY_KUBE3>
AllowedIps = 10.0.1.3/32
Endpoint = 33.33.33.333:51820
EOF
....

*Kube3*

....
cat > /etc/wireguard/wg0.conf <<EOF
[Interface]
Address = 10.0.1.3
PrivateKey = <PRIVATE_KEY_KUBE3>
ListenPort = 51820

[Peer]
PublicKey = <PUBLIC_KEY_KUBE1>
AllowedIps = 10.0.1.1/32
Endpoint = 11.11.11.111:51820

[Peer]
PublicKey = <PUBLIC_KEY_KUBE2>
AllowedIps = 10.0.1.2/32
Endpoint = 22.22.22.222:51820
EOF
....

But as you can see there are some values missing. For that we run this
little script on *Kube1*:

....
for i in 1 2 3; do
  private_key=$(wg genkey)
  public_key=$(echo $private_key | wg pubkey)
  echo "<PRIVATE_KEY_KUBE$i> = $private_key"
  echo "<PUBLIC_KEY_KUBE$i> =  $public_key"
done
....

This gives us a nice list of public and private keys for each node.
Something like this:

....
<PRIVATE_KEY_KUBE1> = MDQgiDU7yPxFwKD9Y1YCYSX+tb4ZGxglXYWLPYlBcm4=
<PUBLIC_KEY_KUBE1> =  oVgxStbC250Mzyl3YJpC0KX0Vo16GafewL44TqwvogM=
<PRIVATE_KEY_KUBE2> = oLHoiKMShKKniUZ6socbBsPQlS9qWkGKCtH6bzdPYEM=
<PUBLIC_KEY_KUBE2> =  bx3qlTronTKYjFNH0JwjbH6tIvUqFeKoMD+5q8pwFXU=
<PRIVATE_KEY_KUBE3> = wLmym9VIrJjHdd97ddoA2W+Kmk5chPRftb/+WyzWFEc=
<PUBLIC_KEY_KUBE3> =  5w1VmcZ+Jh1ews9N2XDP6RnqNzfLY57SQWFuaUUgQ3E=
....

(Don't use these values, use your own!)

Now we have to edit each of those files and copy & paste the actual
values.

Having done that we must update the firewall rules to allow Wireguard
VPN communication:

....
ufw allow in on eth0 to any port 51820
ufw allow in on wg0
ufw reload
....

And finally we start Wireguard and make sure it will be started in the
future as well:

....
systemctl enable --now wg-quick@wg0
....

If you want you can check if everything is configured correctly by
running the following on any of the nodes:

....
wg show
....

It should show something like:

image:images/wgoutput.png[alt_text]

And finally we add some more lines to `/etc/hosts` to make it easier for
us to refer to each node from any node (not sure if it’s required to be
honest, but it's what I did). Just run the following on each node:

....
cat >> /etc/hosts <<EOF
10.0.1.1        kube1
10.0.1.2        kube2
10.0.1.3        kube3
EOF
....

== Installing K8s (using `kubeadm`)

=== Etcd

Execute the following to download and install https://etcd.io/[etcd] on
each server:

....
export ETCD_VERSION="v3.4.7"
mkdir -p /opt/etcd
curl -L https://storage.googleapis.com/etcd/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz \
  -o /opt/etcd-${ETCD_VERSION}-linux-amd64.tar.gz
tar xzvf /opt/etcd-${ETCD_VERSION}-linux-amd64.tar.gz -C /opt/etcd --strip-components=1
....

Now on the each server we’ll create a file so `etcd `will start up at
boot time:

*Kube1*

....
cat > /etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
After=network.target wg-quick@wg0.service

[Service]
Type=notify
ExecStart=/opt/etcd/etcd --name kube1 \
  --data-dir /var/lib/etcd \
  --listen-client-urls "http://10.0.1.1:2379,http://localhost:2379" \
  --advertise-client-urls "http://10.0.1.1:2379" \
  --listen-peer-urls "http://10.0.1.1:2380" \
  --initial-cluster "kube1=http://10.0.1.1:2380,kube2=http://10.0.1.2:2380,kube3=http://10.0.1.3:2380" \
  --initial-advertise-peer-urls "http://10.0.1.1:2380" \
  --heartbeat-interval 200 \
  --election-timeout 5000
Restart=always
RestartSec=5
TimeoutStartSec=0
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
EOF
systemctl enable --now etcd.service
....

*Kube2*

....
cat > /etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
After=network.target wg-quick@wg0.service

[Service]
Type=notify
ExecStart=/opt/etcd/etcd --name kube2 \
  --data-dir /var/lib/etcd \
  --listen-client-urls "http://10.0.1.2:2379,http://localhost:2379" \
  --advertise-client-urls "http://10.0.1.2:2379" \
  --listen-peer-urls "http://10.0.1.2:2380" \
  --initial-cluster "kube1=http://10.0.1.1:2380,kube2=http://10.0.1.2:2380,kube3=http://10.0.1.3:2380" \
  --initial-advertise-peer-urls "http://10.0.1.2:2380" \
  --heartbeat-interval 200 \
  --election-timeout 5000
Restart=always
RestartSec=5
TimeoutStartSec=0
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
EOF
systemctl enable --now etcd.service
....

*Kube3*

....
cat > /etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
After=network.target wg-quick@wg0.service

[Service]
Type=notify
ExecStart=/opt/etcd/etcd --name kube3 \
  --data-dir /var/lib/etcd \
  --listen-client-urls "http://10.0.1.3:2379,http://localhost:2379" \
  --advertise-client-urls "http://10.0.1.3:2379" \
  --listen-peer-urls "http://10.0.1.3:2380" \
  --initial-cluster "kube1=http://10.0.1.1:2380,kube2=http://10.0.1.2:2380,kube3=http://10.0.1.3:2380" \
  --initial-advertise-peer-urls "http://10.0.1.3:2380" \
  --heartbeat-interval 200 \
  --election-timeout 5000
Restart=always
RestartSec=5
TimeoutStartSec=0
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
EOF
systemctl enable --now etcd.service
....

We can test it all works correctly by running:

....
/opt/etcd/etcdctl member list
....

=== Kubernetes

Let’s install the packages necessary to run Kubernetes. We do this by running the following on all nodes:

....
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF | tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl
....

And now on *Kube1*, the master node, we create a configuration file, let’s call it
master-config.yml (just create it anywhere) with the necessary settings for our cluster:

....
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.1
  bindPort: 6443
nodeRegistration:
  name: kube1
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
certificatesDir: /etc/kubernetes/pki
apiServer:
  certSANs:
  - 11.11.11.111
  - api.k8s.example.com
etcd:
  external:
    endpoints:
    - http://10.0.1.1:2379
    - http://10.0.1.2:2379
    - http://10.0.1.3:2379
clusterName: "my-first-cluster-name"
....

And finally let’s create our Kubernetes Control Plane, our master node!

....
kubeadm init --config master-config.yml --node-name master
....

*IMPORTANT*: copy the “kubeadm join ….” command that appears at the end
of kubeadm init output and keep it safe somewhere, we’ll need it for
adding the other nodes to the cluster!

Now we create a symlink to the kube config so kubectl will work:

....
mkdir -p $HOME/.kube
ln -s /etc/kubernetes/admin.conf $HOME/.kube/config
....

Let's see if we can properly run `kubectl`:

....
kubectl version
....

Which should give something like:

image:images/kubectl_version.png[alt_text]

*Yay!!*

So let’s continue because we’re not ready yet.

Now we must set up our pod network, we’ll be using
https://www.weave.works/oss/net/[Weave Net]. So run this on *Kube1*:

....
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
....

After a short while all kinds of new network interfaces should appear.
You can check this by running `ip address`.

Now we need add some persistent routes without which Weave won’t use the
WireGuard VPN tunnels and we also need to allow incoming traffic on the
Weave network:

*Kube1*

....
cat > /etc/systemd/system/overlay-route.service <<EOF
[Unit]
Description=Overlay network route for WireGuard
After=wg-quick@wg0.service

[Service]
Type=oneshot
User=root
ExecStart=/sbin/ip route add 10.96.0.0/16 dev wg0 src 10.0.1.1

[Install]
WantedBy=multi-user.target
EOF
systemctl enable --now overlay-route.service
ufw allow in on weave
ufw reload
....

*Kube2*

....
cat > /etc/systemd/system/overlay-route.service <<EOF
[Unit]
Description=Overlay network route for WireGuard
After=wg-quick@wg0.service

[Service]
Type=oneshot
User=root
ExecStart=/sbin/ip route add 10.96.0.0/16 dev wg0 src 10.0.1.2

[Install]
WantedBy=multi-user.target
EOF
systemctl enable --now overlay-route.service
ufw allow in on weave
ufw reload
....

*Kube3*

....
cat > /etc/systemd/system/overlay-route.service <<EOF
[Unit]
Description=Overlay network route for WireGuard
After=wg-quick@wg0.service

[Service]
Type=oneshot
User=root
ExecStart=/sbin/ip route add 10.96.0.0/16 dev wg0 src 10.0.1.3

[Install]
WantedBy=multi-user.target
EOF
systemctl enable --now overlay-route.service
ufw allow in on weave
ufw reload
....

==== Joining the worker nodes

Now it’s time to add the other nodes to our cluster!

First we make sure the node kubelets
https://propellered.com/posts/kubernetes[get initialized with the correct internal ip]
by running the following on the appropriate nodes:

*Kube1*

....
cat > /etc/default/kubelet <<EOF
KUBELET_EXTRA_ARGS=--node-ip=10.0.1.1
EOF
....

*Kube2*

....
cat > /etc/default/kubelet <<EOF
KUBELET_EXTRA_ARGS=--node-ip=10.0.1.2
EOF
....

*Kube3*

....
cat > /etc/default/kubelet <<EOF
KUBELET_EXTRA_ARGS=--node-ip=10.0.1.3
EOF
....

And now we run the `kubeadm join` command we copied earlier on *Kube2* and *Kube3*.
It will look like this but I've added `--node-name` to it which we’ll set to `kube2` and `kube3`
on the respective nodes:

....
kubeadm join 10.0.1.1:6443 --token <TOKEN> \
--discovery-token-ca-cert-hash sha256:<VERY_LONG_TOKEN> \
--node-name <NODE_NAME>
....

And all nodes should now be up and running! You can run this to check
the nodes:

....
kubectl get nodes
....

Which should look somewhat like this:

image:images/kubectl_getnodes.png[alt_text]

All nodes should show `Ready`.

Let’s also take a look at the pods that are running because that’s
another good way to see if everything is okay:

....
kubectl get pods -A
....

Which should look somewhat like this:

image:images/kubectl_getpods.png[alt_text]

When all pods show “Running” everything is good.

*Yay! We’ve done it!*

==== Troubleshooting

If at any time you feel the need to start all over you can run the
following on each node which will get rid of the local Kubernetes setup
(either from “init” or “join”):

....
kubeadm reset
....

And then on the master node (well actually you can do it on any node)
you can run the following to wipe all data from `etcd` (be careful,
there’s no confirmation prompt!):

....
ETCDCTL_API=3 /opt/etcd/etcdctl del "" --from-key=true
....

And finally get rid of the CNI configuration:

....
rm -rf /etc/cni
....

==== Working remotely

If everything is working as it should it means we're now done working on the nodes directly.
We should be able to do all the remaining work remotely. So run the following on any computer
you want to be able to access your cluster from:

....
scp root@kube1.k8s.example.com:/root/.kube/config ~/.kube
....

If we’ve got `kubectl` installed locally we can now simply use it, like this:

....
kubectl config set-cluster my-first-cluster-name --server=https://api.k8s.example.com:6443
kubectl version
....

And you should see the same output as we saw above.

_From here on out it's assumed that we'll do all the rest of the work in this guide remotely!_

*WARNING*: That Kube Config file gives anyone who has access to it complete control over your
cluster so be careful about who is able to read it or leaving copies lying around!

==== Removing taint

By default our cluster won’t schedule any work on the master node,
reserving it for system applications for security reasons. In our case
we don’t really care about that and we sure don’t want all those
resources to go unused! So run the following to allow pods to run
anywhere (taken from
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#control-plane-node-isolation[Kubernetes
docs]):

....
kubectl taint nodes --all node-role.kubernetes.io/master-
....

You should see something like:

image:images/kubectl_taint.png[alt_text]

=== Dashboard

Now let’s install the
https://github.com/kubernetes/dashboard/blob/master/README.md[Kubernetes
Dashboard], a Web UI you can use to inspect and manage most of the
Kubernetes internals:

....
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
....

But we’ll need to
https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md[create
a user] to be able to log into the UI. Just run the following that will
apply the necessary settings:

....
cat > dashboard-admin-user.yml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF
kubectl apply -f dashboard-admin-user.yml
....

Using the following command you can now get the token to log in with:

....
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') -o jsonpath='{.data.token}' | base64 -d
....

Because we don’t have a proper way yet to access the cluster from “the
outside” we’ll make use of a temporary proxy to connect to our new
dashboard. Run the following comamnd in a terminal on a computer with a
browser and where you have set up kubectl:

....
kubectl proxy
....

We can now access the cluster’s API server which is where the dashboard
is located:

http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

This will show a page like this:

image:images/k8s_dashboard1.png[alt_text]

Where we can paste the token we obtained before. Et voilá, we have
access! You should see something like this:

image:images/k8s_dashboard2.png[alt_text]

=== Helm

Helm is a program for installing applications on Kubernetes, a bit like
package managers like APT, DNF, Brew, etc. We’ll be using it in some of
the next sections so we need to install it. You can find the
instructions here (make sure to install version 3+):

https://helm.sh/docs/intro/install/[https://helm.sh/docs/intro/install/]

Make sure you've installed the default stable repository:

```
helm repo add stable https://kubernetes-charts.storage.googleapis.com/
helm repo update
```

=== Metrics Server

The Dashboard can even show us nice graphs about current usage and such,
and for that we need to install the
https://github.com/kubernetes-incubator/metrics-server[Metrics Server].
We´re going to use the Helm tool we just installed for that. First we add the necessary
Helm repository:

```
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
```

And then we do the actuall installation:

....
helm install \
  -n kube-system \
  metrics-server \
  --set rbac.create=true \
  --set apiService.create=true \
  --set extraArgs.kubelet-preferred-address-types=InternalIP,\
extraArgs.kubelet-insecure-tls=true \
  bitnami/metrics-server
....

(The `extraArgs` settings were necessary in my case or it just wouldn't
connect to any of the nodes. The
https://github.com/bitnami/charts/tree/master/bitnami/metrics-server/[installation
docs] don't really mention it, but see comments on
https://github.com/kubernetes-sigs/metrics-server/issues/167#issuecomment-469914560[this
issue] for more information)

After a couple of minutes the Dashboard should update its interface and
you’ll start to see things like this:

image:images/k8s_dashboard3.png[alt_text]

=== Ingress

So far we haven’t really handled how we will expose our services to the
outside world. We only have a couple of static IP addresses and opening
ports and remembering what IP:port combination works for what service
wouldn’t be very user-friendly. It would also be very inflexible.

So that’s what https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx
[NGINX Ingress Controller] is for, it will allow us to associate hostnames
with our services. We’ve already set up a wild-card *.apps.k8s.example.com
pointing to our *Kube1* node which means that we can use any subdomain
for our services.

It _does_ mean that all outside traffic comes in on a single node,
there’s no load balancing possible here. But realistically our cluster
is never going to be used for high-traffic services, so that’s okay.

It also means that we have to make sure that this ingress controller is
not scheduled like any other service but that it’s _always_ started on
*Kube1*. Again we will use Helm to install things but this time the
configuration is a bit too complex to just pass on the command line, so
first we’ll make a values file:

....
cat > helm-ingress-values.yml <<EOF
controller:
  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Equal
    effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/master
            operator: Exists
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  reportNodeInternalIp: true
EOF
....

And now we do the actual installation:

....
helm install \
  --namespace ingress \
  --create-namespace \
  ingress \
  stable/nginx-ingress \
  -f helm-ingress-values.xml
....

==== Testing external access

Now let see if it actually works (The following steps are adapted from https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/[this page]).

Execute the following lines:

```
kubectl create ns test
kubectl -n test create deployment web --image=gcr.io/google-samples/hello-app:1.0
kubectl -n test expose deployment web --type=NodePort --port=8080
```

Now if we run `kubectl -n test get service web` we should see something like:

```
NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
web       NodePort   10.104.133.249   <none>        8080:31637/TCP   12m
```

But right now the service hasn't been exposed externally yet, so to test it
we must first run the proxy: `kubectl proxy` and then we should be able to
access the service using the following URL:

http://localhost:8001/api/v1/namespaces/test/services/http:web:/proxy/

Ok, great, but now lets use Ingress to access the service from outside the
cluster. For that we must first create the following file:

```
cat > example-ingress.yml <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: web.apps.k8s.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web
                port:
                  number: 8080
EOF
```

And then we apply it using:

```
kubectl -n test apply -f example-ingress.yaml
```

Now if we run `kubectl -n test get ingress` we should see something like this:

```
NAME              CLASS    HOSTS                       ADDRESS   PORTS   AGE
example-ingress   <none>   web.apps.k8s.example.com             80      6m17s
```

Which means everyhing went okay and you should now be able to access the service
using the hostname defined in the file above (the web.apps.k8s.example.com host
won't work, you have to change it for one that works for you. See the linked article
for more information on how to do that).

=== Cert Manager

Now that people can access our apps from the outside world we will want
to protect those communications with proper encryption. For that we’ll
be installing
https://cert-manager.io/docs/installation/kubernetes/[cert-manager] and
configure Letsencrypt support so we can generate certificates on-the-fly
for our apps.

First we need to do some setup:

....
kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.2/cert-manager.crds.yaml
....

Then we add the necessary Helm repository:

....
helm repo add jetstack https://charts.jetstack.io
helm repo update
....

And finally we install the certificate manager:

....
helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v0.14.2
....

Now we just need to tell it where to get the certificates from:

....
cat > letsencrypt-issuer.yml <<EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-testing
  namespace: cert-manager
spec:
  acme:
    # The ACME server URL
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: user@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-testing-key
    # Enable the HTTP-01 challenge provider
    solvers:
    - http01:
        ingress:
          class: nginx
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
  namespace: cert-manager
spec:
  acme:
    # The ACME server URL
    server: https://acme-v02.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: user@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-key
    # Enable the HTTP-01 challenge provider
    solvers:
    - http01:
        ingress:
          class: nginx
EOF
kubectl apply -f letsencrypt-issuer.yml
....

This actually configures two different issuers, one called
“letsencrypt-testing” and the other “letsencrypt”. The difference is
that the former always returns the same fake certificate which can be
used during testing, while the latter actually creates a proper unique
certificate. This is to make sure people don’t go around creating loads
of throw-away certificates.

You can look at the documentation
https://cert-manager.io/docs/installation/kubernetes/#verifying-the-installation[here]
,
https://cert-manager.io/docs/tutorials/acme/ingress/#step-7-deploy-a-tls-ingress-resource[here]
and
https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes[here]
to verify that everything is working correctly.

=== Distributed Storage

The final piece to finish our Kubernetes puzzle is storage. Without that
there’s a lot of apps we couldn’t run. So we’re going to use all the
unused space we have on our nodes, those 180GB partitions we made in the
beginning. Although that setup would never do for production purposes it
is enough for the semi-serious hobbyist.

There are all kinds of storage solutions out there, but not many allow
for these bare-metal setups like we have here. The guide we’re following
uses https://rook.io/[Rook] directly in folders on an already formatted
disk. So I thought this would be perfect for our situation where we have
but a single 200GB disk. Only it turns out that in the newest versions
they deprecated that functionality. Which is why one of the first things
we did was repartition our disk so that, instead of using folders, we
could assign an entire partition exclusively to Rook (well actually
https://rook.io/docs/rook/v1.3/ceph-storage.html[Ceph], but that’s
another story).

==== Set up the Cluster

First we add the necessary Helm repository:

....
helm repo add rook-release https://charts.rook.io/release
helm repo update
....

Then we install the Rook Operator:

....
helm install \
  --namespace rook-ceph \
  --create-namespace \
  rook-ceph \
  rook-release/rook-ceph
....

And then we create our cluster (a minified version of
https://github.com/rook/rook/blob/release-1.3/cluster/examples/kubernetes/ceph/cluster.yaml[this
original] adjusted for our use-case):

....
cat > rook-ceph-cluster.yml <<EOF
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v14.2.9
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mon:
    count: 3
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # port: 8443
    ssl: true
  monitoring:
    enabled: false
    rulesNamespace: rook-ceph
  network:
  rbdMirroring:
    workers: 0
  crashCollector:
    disable: false
  cleanupPolicy:
    deleteDataDirOnHosts: ""
  annotations:
  resources:
  removeOSDsIfOutAndSafeToRemove: false
  storage:
    useAllNodes: false
    useAllDevices: false
    config:
    nodes:
    - name: "kube1"
      devices:
      - name: "sda3"
    - name: "kube2"
      devices:
      - name: "sda3"
    - name: "kube3"
      devices:
      - name: "sda3"
  disruptionManagement:
    managePodBudgets: false
    osdMaintenanceTimeout: 30
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api
EOF
kubectl apply -f rook-ceph-cluster.yml
....

If you want to see if the cluster is up and running correctly you can
install a “toolbox” with some utilities to check the cluster’s status
and health and such. Install it with:

....
kubectl apply -f https://raw.githubusercontent.com/rook/rook/release-1.3/cluster/examples/kubernetes/ceph/toolbox.yaml
....

And connect to it by running:

....
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- bash
....

In the shell that you get you can run commands like:

....
ceph status
ceph osd status
ceph df
rados df
....

Especially `ceph status` is useful to check everything went okay, its
output should look something like this:

....
  cluster:
    id:     ab8bebf3-e8b0-4340-b692-0fe8efce8ff5
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 106m)
    mgr: a(active, since 48s)
    osd: 3 osds: 3 up (since 20s), 3 in (since 20s)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 537 GiB / 540 GiB avail
    pgs:
....

If you don’t get `HEALTH_OK` check
https://rook.io/docs/rook/v1.3/ceph-common-issues.html[Ceph common
issues] for possible solutions.

==== Provision Storage

And finally we need to define a store class. As far as I understand it
what we have so far is just a big bag of bits and no way to access them.
The storage class defines the way (the protocal? the API?) we want to
manage those bits with. In this case we're using
https://rook.io/docs/rook/v1.3/ceph-block.html[Block Storage] which,
AFAIU, shows up as a block device or disk to Pods. These are private to
a Pod and can't / won't be shared. But they are persistent so if a Pod
gets restarted it's data is still there.

To install this storage class and block pool we simply run:

....
cat > rook-ceph-storageclass.yml <<EOF
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 2
    requireSafeReplicaSize: true
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    clusterID: rook-ceph
    pool: replicapool
    imageFormat: "2"
    imageFeatures: layering
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
allowVolumeExpansion: true
reclaimPolicy: Delete
EOF
kubectl apply -f rook-ceph-storageclass.yml
....

There are other storate classes provided by Ceph:
https://rook.io/docs/rook/v1.3/ceph-object.html[Object Storage] (which
is like Amazon S3) and a
https://rook.io/docs/rook/v1.3/ceph-filesystem.html[Shared Filesystem].
We won't discuss those here.

==== Ceph Dashboard

There’s also a Dashboard you can access for this cluster. All the
documentation can be found
https://rook.io/docs/rook/v1.3/ceph-dashboard.html[here]. The “Enable
the Ceph Dashboard” and “Configure the Dashboard” sections can be
skipped because we already did all that. For making the dashboard
publically available we can apply this Ingress:

....
cat > ceph-dashboard-ingress.yml <<EOF
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: rook-ceph-mgr-dashboard
  namespace: rook-ceph
  annotations:
    kubernetes.io/ingress.class: "nginx"
    kubernetes.io/tls-acme: "true"
    # This will use a Fake certificate for testing.
    # Once you're ready to put your service into production
    # change the value below to "letsencrypt" and a real
    # certificate will be provided.
    cert-manager.io/issuer: "letsencrypt-testing"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
    nginx.ingress.kubernetes.io/server-snippet: |
      proxy_ssl_verify off;
spec:
  tls:
   - hosts:
     - rook-ceph-mgr-dashboard.apps.k8s.example.com
     secretName: rook-ceph-mgr-dashboard-tls
  rules:
  - host: rook-ceph-mgr-dashboard.apps.k8s.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: rook-ceph-mgr-dashboard
          servicePort: https-dashboard
EOF
kubectl apply -f ceph-dashboard-ingress.yml
....

_TODO: figure out why the browser still keeps complaining about an
invalid license. I think it's because the internal service also uses TLS
but with a self-signed certificate. Pehaps we should try turning that
off and trust nobody will be able to access that internal service from
the outside anyway_

=== OAuth2

_WARNING: This part is still a work in progress. It works but I'm not really happy with it._

With the final piece installed it doesn't mean we're finished of course. There's
still a lot of other things we could install. One very important part is authentication
and authorization. If our cluster is publically accessible we don't want to just let
anyone inside!

For now we're going to install a fairly simple component called
https://oauth2-proxy.github.io/oauth2-proxy/[oauth2-proxy]. I basically followed
the instructions from this article
https://www.digitalocean.com/community/tutorials/how-to-protect-private-kubernetes-services-behind-a-github-login-with-oauth2_proxy[How to Protect Private Kubernetes Services Behind a GitHub Login with oauth2_proxy].
But some minor things had to be changed to make things work.

For this guide I'm going to use GitHub authentication because it's an account that most
people I'd give acces to my cluster to will have and the setup is really easy. I got
stuck on the Google authentication setup because the docs are out-of-date and I wasn't
able to make things work (issue opened, will update the guide if I get things to work in
the future).

Let's first start by creating a namespace (normally I do this as part of the Helm install
command, but here I want to make sure some other things are set up first, so we do
it manually):

```
kubectl create ns oauth2
```

See https://oauth2-proxy.github.io/oauth2-proxy/auth-configuration#github-auth-provider[GitHub Auth Provider]
for instructions on how to set up a GitHub OAuth application. Make sure you copy the Client ID
and Client Secret for use in the next step.

We also need a Cookie which you can generate here: https://generate.plus/en/base64

Next we need to create a secret holding those values:

```
kubectl -n oauth2 create secret generic oauth2-proxy-creds \
  --from-literal=cookie-secret=<GENERATED_COOKIE> \
  --from-literal=client-id=<GITHUB_CLIENT_ID> \
  --from-literal=client-secret=<GITHUB_CLIENT_SECRET>
```

Now we need to create our configuration file, eg `oauth2-proxy-helm-values.yml`:

```
config:
  existingSecret: oauth2-proxy-creds

extraArgs:
  whitelist-domain: .apps.k8s.example.com
  cookie-domain: .apps.k8s.example.com
  provider: github

authenticatedEmailsFile:
  enabled: true
  restricted_access: |-
    tako@example.com
    someone@example.com
    example@gmail.com

ingress:
  enabled: true
  path: /
  hosts:
    - auth.apps.k8s.example.com
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt
  tls:
    - secretName: oauth2-proxy-https-cert
      hosts:
        - auth.apps.k8s.example.com
```

Make sure to use the host names particular to your use-case. The email addresses in
the `restricted_access` option are the people that will be granted access.

Now let's install the https://hub.helm.sh/charts/stable/oauth2-proxy[oauth2-proxy chart] itself:

```
helm install -n oath2 oath2-proxy stable/oauth2-proxy -f oauth2-proxy-helm-values.yml
```
